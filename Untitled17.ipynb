{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re,nltk\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Lambda, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us fix up the target as categories to start with# Let us  \n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('E:/VINOD KUMAR/OMEGA_PROJECT_NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data=pd.read_excel(\"Train_data_IPM Jan,2017-May,2018.xlsx\")\n",
    "test_data=pd.read_excel(\"IPM June1-17, 2018.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(input_string):\n",
    "    input_string=re.sub(r'[^a-zA-Z0-9\\s]', '',input_string)\n",
    "    sentence = nltk.tokenize.sent_tokenize(input_string)\n",
    "    out = []\n",
    "    for sent in sentence:\n",
    "        wordTokens = nltk.tokenize.word_tokenize(sent)\n",
    "        lower_tokens = [token.lower() for token in wordTokens]\n",
    "        stop = stopwords.words('english')\n",
    "        tokens = [token for token in lower_tokens if token not in stop]\n",
    "        lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "        tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "        out.append(\" \".join(tokens))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data[\"schedule_note\"]=actual_data[\"schedule_note\"].astype(\"str\")\n",
    "test_data[\"schedule_note\"]=test_data[\"schedule_note\"].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=[]\n",
    "for i in range (0,len(actual_data)):\n",
    "        sentence.append(process_text(actual_data[\"schedule_note\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ts=[]\n",
    "for i in range (0,len(test_data)):\n",
    "        sentence_ts.append(process_text(test_data[\"schedule_note\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data[\"sentence\"]=pd.DataFrame(sentence)\n",
    "test_data[\"sentence\"]=pd.DataFrame(sentence_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data[\"sentence\"]=actual_data[\"sentence\"].astype('str')\n",
    "actual_data[\"Categories1\"]=actual_data[\"Categories1\"].astype(\"category\")\n",
    "actual_data[\"Sub_categories1\"]=actual_data[\"Sub_categories1\"].astype(\"category\")\n",
    "test_data[\"sentence\"]=test_data[\"sentence\"].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the categories to label\n",
    "actual_data[\"Categories2\"] = le.fit_transform(actual_data.Categories1)\n",
    "\n",
    "# Converting the subcategories to labels# Convert \n",
    "actual_data[\"Sub_categories2\"] = le.fit_transform(actual_data.Sub_categories1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the categories to label\n",
    "test_data[\"Categories2\"] = le.fit_transform(test_data.Categories1)\n",
    "\n",
    "# Converting the subcategories to labels# Convert \n",
    "test_data[\"Sub_categories2\"] = le.fit_transform(test_data.Sub_categories1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tran_id</th>\n",
       "      <th>tran_date</th>\n",
       "      <th>forwhom_id</th>\n",
       "      <th>schedule_note</th>\n",
       "      <th>Categories1</th>\n",
       "      <th>Sub_categories1</th>\n",
       "      <th>Previous_Appointment</th>\n",
       "      <th>Categories2</th>\n",
       "      <th>Sub_categories2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>672852</td>\n",
       "      <td>2016-07-08</td>\n",
       "      <td>30570</td>\n",
       "      <td>conf nc</td>\n",
       "      <td>APPOINTMENTS</td>\n",
       "      <td>NEW APPOINTMENT</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>673452</td>\n",
       "      <td>2016-07-11</td>\n",
       "      <td>3603</td>\n",
       "      <td>lvm, NN</td>\n",
       "      <td>MISCELLANEOUS</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>673800</td>\n",
       "      <td>2016-07-12</td>\n",
       "      <td>22281</td>\n",
       "      <td>conf nc</td>\n",
       "      <td>APPOINTMENTS</td>\n",
       "      <td>NEW APPOINTMENT</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>674558</td>\n",
       "      <td>2016-07-13</td>\n",
       "      <td>24646</td>\n",
       "      <td>Records being sent to ANTIOCH</td>\n",
       "      <td>MISCELLANEOUS</td>\n",
       "      <td>SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>676145</td>\n",
       "      <td>2016-07-18</td>\n",
       "      <td>22072</td>\n",
       "      <td>conf nc</td>\n",
       "      <td>APPOINTMENTS</td>\n",
       "      <td>NEW APPOINTMENT</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tran_id   tran_date  forwhom_id                   schedule_note  \\\n",
       "0   672852  2016-07-08       30570                        conf nc    \n",
       "1   673452  2016-07-11        3603                         lvm, NN   \n",
       "2   673800  2016-07-12       22281                        conf nc    \n",
       "3   674558  2016-07-13       24646  Records being sent to ANTIOCH    \n",
       "4   676145  2016-07-18       22072                        conf nc    \n",
       "\n",
       "     Categories1                                Sub_categories1  \\\n",
       "0   APPOINTMENTS                                NEW APPOINTMENT   \n",
       "1  MISCELLANEOUS                                         OTHERS   \n",
       "2   APPOINTMENTS                                NEW APPOINTMENT   \n",
       "3  MISCELLANEOUS  SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)   \n",
       "4   APPOINTMENTS                                NEW APPOINTMENT   \n",
       "\n",
       "  Previous_Appointment  Categories2  Sub_categories2  \n",
       "0                   No            0                7  \n",
       "1                   No            3                8  \n",
       "2                   No            0                7  \n",
       "3                   No            3               16  \n",
       "4                   No            0                7  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tran_id</th>\n",
       "      <th>tran_date</th>\n",
       "      <th>forwho_id</th>\n",
       "      <th>schedule_note</th>\n",
       "      <th>Categories1</th>\n",
       "      <th>Sub_categories1</th>\n",
       "      <th>Previous_Appointment</th>\n",
       "      <th>Categories2</th>\n",
       "      <th>Sub_categories2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1780357</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>57998</td>\n",
       "      <td>Date &amp; Time of call: 6/14/18 @ 3:59''Ordering ...</td>\n",
       "      <td>APPOINTMENTS</td>\n",
       "      <td>NEW APPOINTMENT</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1774218</td>\n",
       "      <td>2018-06-11</td>\n",
       "      <td>106916</td>\n",
       "      <td>FTR. Left Shoulder Subacomial Decompression wi...</td>\n",
       "      <td>APPOINTMENTS</td>\n",
       "      <td>NEW APPOINTMENT</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1775725</td>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>97415</td>\n",
       "      <td>1/12 Pri Cigna: Active,  Open Access Plus  Pla...</td>\n",
       "      <td>MISCELLANEOUS</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1775709</td>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>48267</td>\n",
       "      <td>Pri BS: Active, PPO Plan, IN N/W, Ref is not r...</td>\n",
       "      <td>MISCELLANEOUS</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1772813</td>\n",
       "      <td>2018-06-08</td>\n",
       "      <td>23202</td>\n",
       "      <td>''Date &amp; time of call: 06/08-@9:16 AM       ''...</td>\n",
       "      <td>MISCELLANEOUS</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tran_id  tran_date  forwho_id  \\\n",
       "0  1780357 2018-06-14      57998   \n",
       "1  1774218 2018-06-11     106916   \n",
       "2  1775725 2018-06-12      97415   \n",
       "3  1775709 2018-06-12      48267   \n",
       "4  1772813 2018-06-08      23202   \n",
       "\n",
       "                                       schedule_note    Categories1  \\\n",
       "0  Date & Time of call: 6/14/18 @ 3:59''Ordering ...   APPOINTMENTS   \n",
       "1  FTR. Left Shoulder Subacomial Decompression wi...   APPOINTMENTS   \n",
       "2  1/12 Pri Cigna: Active,  Open Access Plus  Pla...  MISCELLANEOUS   \n",
       "3  Pri BS: Active, PPO Plan, IN N/W, Ref is not r...  MISCELLANEOUS   \n",
       "4  ''Date & time of call: 06/08-@9:16 AM       ''...  MISCELLANEOUS   \n",
       "\n",
       "   Sub_categories1 Previous_Appointment  Categories2  Sub_categories2  \n",
       "0  NEW APPOINTMENT                   No            0                5  \n",
       "1  NEW APPOINTMENT                   No            0                5  \n",
       "2           OTHERS                   No            3                6  \n",
       "3           OTHERS                   No            3                6  \n",
       "4           OTHERS                   No            3                6  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['tran_id', 'tran_date', 'forwhom_id', 'schedule_note', 'Categories1',\n",
       "        'Sub_categories1', 'Previous_Appointment', 'Categories2',\n",
       "        'Sub_categories2'],\n",
       "       dtype='object'),\n",
       " Index(['tran_id', 'tran_date', 'forwho_id', 'schedule_note', 'Categories1',\n",
       "        'Sub_categories1', 'Previous_Appointment', 'Categories2',\n",
       "        'Sub_categories2'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.columns, test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=actual_data[['schedule_note', 'Categories2','Sub_categories2']]\n",
    "test=test_data[['schedule_note','Categories2','Sub_categories2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56143\n"
     ]
    }
   ],
   "source": [
    "#Each word frequency count in Whole Corpus\n",
    "freq_count_data = pd.Series(' '.join(data['schedule_note'].astype(str)).split()).value_counts()\n",
    "print(len(freq_count_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = data.pop('Categories2')\n",
    "y2 = data.pop('Sub_categories2')\n",
    "X = data.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ty1 = test.pop('Categories2')\n",
    "Ty2 = test.pop('Sub_categories2')\n",
    "TX = test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['conf nc '],\n",
       "        ['lvm, NN'],\n",
       "        ['conf nc '],\n",
       "        ...,\n",
       "        ['-'],\n",
       "        ['-'],\n",
       "        ['-']], dtype=object), '##########', (130535, 1), '#########', 0    0\n",
       " 1    3\n",
       " 2    0\n",
       " 3    3\n",
       " 4    0\n",
       " Name: Categories2, dtype: int64)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X ,\"##########\",X.shape,\"#########\",y1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX = np.array(TX).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX=TX.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['conf nc ', 'lvm, NN', 'conf nc ', ..., '-', '-', '-'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Date & Time of call: 6/14/18 @ 3:59''Ordering Provider: Dr. Ganga''Caller: Dr. Loubser''Reviewing MD: Dr. Loubser''Px name: Ulloa, Jennifer  (PT00052573)''CB No: 713-301-8223''Due Date: 6/15/18 @ 5pm''Re: Sympathetic lumbar block  6/15 left vm Dr. Kwaan leaving 12:15 and Dr. Ganga away until 7/5-to call to the office to make new pper to peer after that-dm\",\n",
       "       'FTR. Left Shoulder Subacomial Decompression with Rotator Cuff Repair and mumford open Procedure @ St.Francis at 2:00pm.Surgical Assistant Veronica Gonzalez conf 06/08 via email, Pre-Op @ Dr.Cohen Office 06/08/18 at 9:45am, Follow up at DC office 06/06/18 at 3:45pm. Post-Op @ SF office 6/22/18 at 9:00am. Cold therapy Unit RFA sent to comp smart 06/06/18.',\n",
       "       '1/12 Pri Cigna: Active,  Open Access Plus  Plan, In N/w, Ref is not req, Auth is not req for Acupuncture--> $20 Spl Copay, No Ded, OOP $4,500, Met $2,829.85, Rem $1,670.15, Plan covers @ 100% after the Copay, Pt has 12 Visits per year not met at this time, Ref# 1075 Ruth E.<Satish 06/04/18>',\n",
       "       ..., '-', '5', '3'], dtype='<U357')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 33592\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings is a concept of turning words into fixed vectors. \n",
    "We load a text file from /home/datasets/glove.6B/glove.6B.100d.txt \n",
    "where each line contains a word and a 100 length vector that it corresponds to. \n",
    "The glove vectors themselves are based off Stanford's research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24159 unique tokens.\n",
      "Shape of data tensor: (130535, 1000)\n",
      "Shape of label tensor: (130535, 5)\n",
      "Shape of label tensor: (130535, 19)\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(y1))\n",
    "labels2 = to_categorical(np.asarray(y2))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print('Shape of label tensor:', labels2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the train and validation datasets\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "labels2 = labels2[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples] # This is for categories as target\n",
    "y_train2 = labels2[:-num_validation_samples] # This is for subcategories as target\n",
    "\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:] # This is for categories as target\n",
    "y_val2 = labels2[-num_validation_samples:] # This is for subcategories as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 50\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimension))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector[:embedding_dimension]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24160, 50)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130535, 1000)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# num_words is the number of unique words\n",
    "# Embedding dimension is the dimension of the hidden layer that we choose --> embedding vector ( we can choose this to be 50, or 100 or so)\n",
    "# input length is the fixed length of the sentence that we feed to this embedding layer network ( https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work)\n",
    "# Here we mentioned trainable = false, because we are directly using the weights from embedding matrix from glove. \n",
    "model.add(embedding_layer)\n",
    "#model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Hidden layer 1\n",
    "model.add(Dense(50))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Hidden layer 2\n",
    "model.add(Dense(25))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\keras\\callbacks.py:999: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "              patience=5, min_lr=0.00001, verbose=1, epsilon=0.001)\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=1, mode='auto')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 104428 samples, validate on 26107 samples\n",
      "Epoch 1/10\n",
      "104428/104428 [==============================] - 167s 2ms/step - loss: 0.5082 - acc: 0.8198 - val_loss: 0.4718 - val_acc: 0.8504\n",
      "Epoch 2/10\n",
      "104428/104428 [==============================] - 167s 2ms/step - loss: 0.3699 - acc: 0.8713 - val_loss: 0.3765 - val_acc: 0.8753\n",
      "Epoch 3/10\n",
      "104428/104428 [==============================] - 180s 2ms/step - loss: 0.3351 - acc: 0.8835 - val_loss: 1.2257 - val_acc: 0.3660\n",
      "Epoch 4/10\n",
      "104428/104428 [==============================] - 177s 2ms/step - loss: 0.3120 - acc: 0.8925 - val_loss: 0.8309 - val_acc: 0.8404\n",
      "Epoch 5/10\n",
      "104428/104428 [==============================] - 172s 2ms/step - loss: 0.3006 - acc: 0.8957 - val_loss: 0.9136 - val_acc: 0.8355\n",
      "Epoch 6/10\n",
      " 32256/104428 [========>.....................] - ETA: 1:53 - loss: 0.2826 - acc: 0.9021"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[reduce_lr, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LSTM instead of MLP\n",
    "Hence forward we will be using plain embeddings instead of glove word2vec as it has not given great performance ( Suspect words like Rx etc., are being omitted from glove vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "# num_words is the number of unique words\n",
    "# Embedding dimension is the dimension of the hidden layer that we choose --> embedding vector ( we can choose this to be 50, or 100 or so)\n",
    "# input length is the fixed length of the sentence that we feed to this embedding layer network ( https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work)\n",
    "# Here we mentioned trainable = false, because we are directly using the weights from embedding matrix from glove. \n",
    "#model.add(embedding_layer)\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length= data.shape[1] ))\n",
    "#model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "#model.add(Flatten())\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "              patience=5, min_lr=0.00001, verbose=1, epsilon=0.001)\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=1, mode='auto')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[reduce_lr, early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Convolution net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
